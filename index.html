<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="./css/style.css" />
		<link rel="shortcut icon" href="./img/kc-logov2.ico" type="image/x-icon" />
		<title>Kókány Corporations</title>
	</head>
	<body>
		<div class="container">
			<div class="search">
				<div class="logo-header">
					<img src="./img/kc-logov2.png" alt="Kókány Corporations Logo" />
					<h3>Kókány Corporations</h3>
					<h5>Team Description Material</h5>
				</div>
				<div class="links" id="toc"></div>
			</div>
			<div class="main-text">
				<div>
					<h1>Logisctical Informations</h1>
					<h3><b>Team name:</b> Kókány Corporations</h3>
					<h3>
						<b>Organistaion:</b> Nyíregyházi SZC Széchenyi István Technikum és
						Kollégium
					</h3>
					<h3><b>Country:</b> Hungary</h3>
					<h3><b>Mentor:</b> Mr. András Bakti</h3>
					<h3><b>Contact Person:</b> Mr. András Bakti</h3>
					<h3><b>Phone:</b> +3630/252-3931</h3>
					<h3><b>Email:</b> info@robottep.hu</h3>
					<h3>
						<b>Website:</b>
						<a href="https://robottep.hu" target="_blank"
							>Our mentor's website</a
						>
					</h3>
					<p>
						Our team consists of 4 ambitious members. We created this team in
						2022, since then we have tried our best to participate in various
						competitions and challenge ourselves with more complicated
						projects. We all plan to go and study things related to robotics.
                        We all plan to attend to university
						and learn more. We are good friends with each other and we
						try to help one another whenever we can. It is a mutual relationship between
						us, team mebers. If one of us doesn't understand a concept, we discuss it
                        and move forward with the development.
					</p>
				</div>
				<div class="margintop">
					<h1>Members of the team</h1>
					<div class="introduc">
						<h3>Mr. András Bakti</h3>
						<img src="img/pfp6.jpg" alt="Our mentor pfp" />
						<h4>Role: Mentor</h4>
						<p>
							Mr. Bakti András is a teacher at our school and our mentor. He focuses on finding sponsors.
                            He also gives us advice about the robot's physical aspects and he will be
							responsible for our safety during our stay in Eindhoven.
						</p>
					</div>
					<div class="introduc">
						<h3>Máté Mester</h3>
						<img src="img/pfp1.jpg" alt="Máté" />
						<h4>Role: Hardware</h4>
						<p>
							Máté focuses on the mechanical aspect of our robot, he works on
							the design and assembly of the robot. He's 18 years old and he designs the main body,
							and he designed our latest 3D printed wheels. He also deals with
							the logistical issues for the team. This is his third year with
							the team. Máté took interest in mechanical engineering since he's
                            joined the team.
						</p>
					</div>
					<div class="introduc">
						<h3>Tran Duy Dat</h3>
						<img src="img/pfp3.jpg" alt="Robi" />
						<h4>Role: Hardware</h4>
						<p>
							Dat also focuses on the mechanical aspect of the robot, he designs
							parts using CAD software and helps with the assembly of parts. He
							designed our custom support arm which enables the robot to
							climb steep surfaces. He's 18 years old as well and he has taken
							interest in mechanical engineering and web development. He's the
							one who developed the TDM as a website and designed the team
							shirt. He's also in his third year in this team.
						</p>
					</div>
					<div class="introduc">
						<h3>Bence Vadász</h3>
						<img src="img/pfp4.jpg" alt="Bence" />
						<h4>Role: Hardware</h4>
						<p>
							Bence is the newest and youngest member of our team, he helps out
							with the hardware related aspects of our robot and the assembly.
							He's the one who designed our team's logo.
						</p>
					</div>
					<div class="introduc">
						<h3>Zsolt Vadász</h3>
						<img src="img/pfp5.jpg" alt="Zsolt" />
						<h4>Role: Programmer</h4>
						<p>
							Zsolt is responsible for the software of the robot. He's the one
							who programmed the robot's movement and arm functionalities as well
                            as its image recognition software. He's interested in micro controller
							programming as well as the manufacturing process of said
							controllers. He's 19 years old.
						</p>
					</div>
				</div>
				<div class="margintop">
					<h1>Competitions we have attended until now</h1>
					<div class="comps-box">
						<h3>MIRK 2022</h3>
						<img
							src="img/firstPlace3.jpg"
							alt="Picture of the 2022 MIRK RMRC robot"
						/>
						<p>
							Our first competition was in March 2022. As you can see the robot
							was made out of 2 3mm thick plexiglass sheets which were cut with
							laser. The arm was controlled with a PCA9685 PWM controller and
							our robotic arm used
							<a href="https://www.towerpro.com.tw/product/sg90-7/">SG90</a>
							servos. We used 3 salvaged 18650 cells connected in series to
							provide 12V that we stepped down to 5V for the Raspberry Pi 4 and
							the PCA9685. We used the cheapest hobby DC motors we had on hand,
							which were controlled with a MotoZero and ran on 12V.
							unfortunately the motors broke down during the competition and we
							couldn't reuse them in the following year. Our robotic arm built
                            using servos worked well, and got us enough points to win.
                            We controlled the robot with a PlayStation 4 controller,
                            the code was written in Python and can be viewed in this
							<a href="https://github.com/zsoltiv/robocup">repository</a>,
						</p>
					</div>
					<div class="comps-box">
						<h3>MIRK 2023</h3>
						<img
							src="./img/firstPlace2.jpg"
							alt="Picture of the 2023 MIRK RMRC robot"
						/>
						<p>
							We participated in MIRK the next year as well, for which
							we used better motors and bigger wheels, which
							helped us in the uneven parts of the arena. We used a different
                            arm design, which unfortunately did not work.
                            Half of our motors were metal geared and the other half
							plastic geared. This was a big improvment because they didn't
							break this time. After winning the championship, our mentor, Mr.
							András Bakti encouraged us to test ourselves at RoboCup23.
						</p>
					</div>
					<div class="comps-box">
						<h3>RoboCup 2023</h3>
						<img
							src="img/robocup23.jpg"
							alt="Picture of the robot in RoboCup23"
						/>
						<p>
							RoboCup 2023 in Bordeaux, France, was our first international
							competition. Our main improvement was going to be our custom
                            robotic arm which we had been working on since MIRK.
						</p>
						<p>
                            The arm was meant to use 28BYJ-48 stepper motors all driven with a
                            custom PCB with a couple of
                            <a href="https://www.hestore.hu/prod_getfile.php?id=15411">GPIO chips</a> connected to
                            <a href="https://www.hestore.hu/prod_getfile.php?id=11958">darlington transistor arrays</a>
                            providing enough IO for up to 8 stepper motors.
                            However we soon dropped our custom PCB idea as having the design
                            manufactured was costly and also required soldering SMD components
                            with which the team had no prior experience. We ended up using a
                            single GPIO extender IC (also MCP23017 but in a through-hole package),
                            and we reduced the number of actuators in our arm. In the end,
                            our arm still did not work. The details are in a later section.
						</p>
						<p>
							The sockets for the motors were 3D printed and the rough
							conditions were too much for the basic L shape motor holders. One
							of them broke in the middle of the competition.
						</p>
						<p>
							An other challenge was the camera streaming and the image
							recognition, the latter of which was not working either due to
                            a lack of experinece with machine learning. Zsolt had
							spent a great amount of time on debugging and programming.
						</p>
						<p>
							All in all the competition was really helpful and we learned a lot
							from it.
						</p>
					</div>
					<div class="comps-box">
						<h3>MIRK 2024</h3>
						<img
							src="./img/firstPlace1.jpg"
							alt="Picture of the 2024 MIRK RMRC robot"
						/>
						<p>
							This was our third time competing at MIRK and our third win in
							Hungary. With this we were three times Hungarian champions.
						</p>
						<p>
							Kókánybot had come a long way. We moved our motors next to
							our electronics, decreasing the robot's height and optimizing the
							weight distribution. We switched the energy source from salvaged
							18650 lithium-ion batteries to a 20V Parkside drill battery,
							however we had to add an extra buck converter. We also
							switched back to using MotoZero from the previously used L298N
							board due to space constraints, however the MotoZero could easily
							overheat, so we needed to install a fan and put a heatsink on it
							to prevent the circuits from overheating. We switched from steppers
							back to servo motors. We had also realized that the 3D printed arm
							wasn't durable enough so we used some metal parts we had on hand
							and built an arm out of those. For driving the arm, we used
							<a href="https://www.adafruit.com/product/815">Adafruit's PCA9685 board</a>.
                            We also fixed the camera latency issues and intalled a second camera for
                            on the robot's rear end.
						</p>
						<p>
							We had two operators. One was driving robot and the other was
							assisting with the cameras. We had brought a secondary monitor
							with us to see the camera output better.
						</p>
					</div>
				</div>
				<div class="margintop">
					<h1>Software</h1>
					<div>
						<h2>Human-robot interface</h2>
						<p>
							We can interact with our robot via any laptop or desktop computer
							(although the latter isn't very practical), using either a Wireless or
							Ethernet (our choice) connection.
						</p>
						<p>
							There are two main programs for interacting with Kókánybot:
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/master/src/main.c"
								><code>kokanyctl</code></a
							>
							and
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/master/kokanyrecognize.py"
								><code>kokanyrecognize</code></a
							>. The former is used for controlling the robot's actuators and
							performing QR code recognition, while the latter uses our custom
							image-recognition model to recognize hazmat signs. The details of
							the programs and the connection are elaborated in the
							<i>Communication</i> section.
						</p>
						<p>
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/master/src/main.c"
								><code>kokanyctl</code></a
							>
							takes keyboard input and sends it to
							<a
								href="https://github.com/zsoltiv/kokanybot/blob/master/src/main.c"
								><code>a daemon</code></a
							>
							running on Kókánybot which processes the raw input into usable
							events.
						</p>
					</div>
					<div>
						<h2>Simplifying our software with device trees</h2>
						<p>
							Last year we used a library called
							<a
								href="https://git.kernel.org/pub/scm/utils/i2c-tools/i2c-tools.git/about/"
							>
								libi2c</a
							>
							to control some of our stepper motors using an external
							<a href="https://www.microchip.com/en-us/product/MCP23017"
								>MCP23017 GPIO expander</a
							>. Since then we have switched to using servos for our robotic
							arm. We also wanted to make our software more flexible, and the
							best way to do this was ripping out the useless code and using the
							right tools for the job.
						</p>
						<p>
							While programs are allowed to just use libi2c, it adds (often
							redundant) extra code to a project, not to mention a lot of extra
							complexity since low-level driver code is not trivial. The Kernel
							actually has drivers for a lot of common ICs such as GPIO
							expanders—like the MCP23017 we used—or PWM controllers like the
							<a href="https://www.adafruit.com/product/815">PCA9685</a>.
						</p>
						<p>
							One improvement we've made was deleting the I2C parts from our
							code, and instead using the
							<a
								href="https://elixir.bootlin.com/linux/latest/source/drivers/pwm/pwm-pca9685.c"
								>kernel driver</a
							>
							instead. This is great because we can just interact with the
							userspace PWM API via something like
							<a href="https://github.com/zsoltiv/libhwpwm/tree/master"
								>libhwpwm</a
							>
							(more on that later).
						</p>
						<p>
							To achieve this, we wrote a
							<a
								href="https://github.com/zsoltiv/kokanybot/blob/master/overlays/kokanyservoctl.dts"
								>device tree overlay</a
							>. Device tree overlays are kind of like patch files for
							<a
								href="https://www.kernel.org/doc/html/latest/devicetree/usage-model.html"
								>device trees</a
							>. This file allows us to tell Linux what chips are available on
							certain I2C addresses.
						</p>
					</div>
					<div>
						<h2>Switching from TCP to UDP</h2>
						<p>
							Robotics taught us just how fragile computers are when subjected
							to harsher environments. Short circuits may occur, components
							might get knocked against a robot's frame, boards can overheat,
							all of which can result in a robot rebooting or shutting down
							entirely.
						</p>
						<p>
							In our previous competitions, we have <i>always</i> had faults
							like these occur. If things were going too well, then our UTP
							cable slipped out of our control station's Ethernet port. In these
							cases, we almost always had to reboot our robot and do the
							software side of the setup again and then reconnect, wasting
							precious time.
						</p>
						<p>
							Mitigating these issues while sticking with TCP sockets would mean
							having to handle potential connection issues every time we
							send/receive data. Instead of doing that, we've switched to UDP,
							which is a different communications protocol. As of now, none of
							our programs contain any any TCP related code making all
							networking stateless (the multimedia bits <b>might</b> be stateful
							but FFmpeg deals with all of that).
						</p>
						<p>
							The main advantage of UDP compared to TCP is its statelessness:
							networked programs do not need to
							<code>connect()</code>, <code>listen()</code> nor
							<code>accept()</code>, which also means no need to track clients;
							you create a socket, and send/receive data over it using
							<code>sendto()</code> and <code>recvfrom()</code>. This eliminates
							the need for handling reconnections. This is great because our
							custom network code sends only a stream of octets (detailed in the
							next section), which always correspond to a keypress or key
							release.
						</p>
						<p>
							The main downsides of UDP are that it does not guarantee data
							packets arriving in the correct order nor does it guarantee that
							they arrive <i>at all</i>. This is not really an issue for us,
							since the entire network is point-to-point with no more than two
							hosts, and we have yet to experience any problems.
						</p>
					</div>
					<div>
						<h2>Network arcitecture</h2>
						<p>
							Kókánybot has to transport two kinds of data: keyboard input and
							video data. There are two ways to go about transporting them: we
							could have chosen to multiplex the input and the video and data
							streams (similar to
							<a href="https://stackoverflow.com/a/38727112">this SO answer</a>
							implemented using the API). We chose the path of least resistance
							though, and went with separate connections for each video feed and
							input. Currently there are
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/03646de81618e50c38f7d2d41fce398d238cef72/src/main.c#L39"
							>
								two camera feeds
							</a>
							and a
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/03646de81618e50c38f7d2d41fce398d238cef72/src/main.c#L32"
							>
								connection for transporting keycodes </a
							>. This allows us to avoid a lot of headaches and keep the
							networking as stateless as possible.
						</p>
						<p>
							We don't use any complicated protocols for data transmission,
							because we'd liked to keep the processing needs as low as
							possible.
						</p>
					</div>
					<div>
						<h2>Efficiently encoding keyboard input for low overhead</h2>
						<p>
							Keys on a keyboard usually correspond to
							<a
								href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/input-event-codes.h"
								>scancodes</a
							>
							in software. These values do not necessarily correspond to
							characters, since they're the raw values of keys, and do not take
							the user's layout into account. SDL2 allows us to easily convert
							scancodes to keycodes which
							<b>do</b> correspond to actual characters a user may type in. The
							macro used to this conversion is
							<code>SDL_SCANCODE_TO_KEYCODE()</code>.
						</p>
						<p>
							Sending keycodes over the wire is part of the solution, but we
							also need to send the state of the key (whether it's pressed or
							not), ideally in a single byte.
						</p>
						<p>
							A key can have two states meaning, two possible values, so it can
							be represented in log<sub>2</sub>2=1 bits. This leaves 7 bits to
							represent keycodes, but there are more than 2<sup>7</sup> possible
							values. However, we only handle WASD and numeric keys. Anything
							else is ignored on the server side.
						</p>
						<p>
							We use some bit manipulation to encode and decode the actual data:
							<a
								href="https://github.com/zsoltiv/kokanyctl/blob/e4ad1f0c0d85028a7224afd44b5ab4a433d12c42/src/net.c#L119"
							>
								<code>net_encode_scancode()</code>
							</a>
							in <code>kokanyctl</code> encodes the state into the most
							significant bit of the octet by shifting a boolean (<code
								>false</code
							>
							and <code>true</code> correspond to 0 and 1 in C) 7 bits to the
							left (<code>pressed &lt;&lt; 7</code>) and then we bitwise OR it
							with the keycode.
						</p>
						<p>
							On the receiving end, <code>kokanybot</code> processes the
							incoming data in
							<a
								href="https://github.com/zsoltiv/kokanybot/blob/464a93d76f1663cc1291ffcf36dc81e000aed12f/src/input.c#L42"
							>
								<code>input_process_key_event</code> </a
							>. This calls an inline function,
							<code>is_key_pressed</code> which uses a boolean trick (in C,
							everything aside from <code>0</code> is <code>true</code>, so two
							boolean NOTs result in 0 or 1 if the value is anything but zero).
							<code>0x80</code> corresponds to 128 in hexadecimal, which
							corresponds to the most significant bit (10000000<sub>(2)</sub>),
							where we stored the state of the key. bitwise ANDing the keycode
							with 128 produces either 0 or 128, and this is what we convert to
							either 0 or 1 with the previous boolean trick. Afterwards we
							discard the most significant bit, because the eight bit represents
							the key state and isn't part of the keycode itself. We then
							<a href="https://en.wikipedia.org/wiki/Lookup_table">look up</a>
							the correct function.
						</p>
					</div>
					<div>
						<h2>Removing gas sensing code</h2>
						<p>
							The rulebook's
							<a
								href="https://oarkit.intelligentrobots.org/home/wp-content/uploads/2023/10/RoboCupRescue-RMRC-2024-Rulebook-working-draft-2023-10-15.pdf"
								>latest draft</a
							>
							does not mention CO<sub>2</sub> sensing, therefore we've removed
							all code related to it. If a later draft brings it back, we can
							just reuse a previous commit, as we've often done during
							development.
						</p>
					</div>
					<div>
						<h2>Training an object detection model</h2>

						<p>
							Last time, one of the most challenging aspects of the competition
							was the object detection feature robots needed. Back in Bordeaux,
							we failed to detect anything and got 0 points for object
							detection. We definitely needed to improve on that.
						</p>

						<p>
							Firstly, we found a
							<a
								href="https://universe.roboflow.com/new-workspace-xqnz7/rmrc-dqa9p"
								>large enough dataset</a
							>
							on the internet (1k+ photos), which was a godsend because it
							spared us from having to manually take pictures and tag them.
						</p>

						<p>
							Secondly, we switched from the outdated YoloV5 to the up-to-date
							YoloV8 which is supposedly better in every single way. We chose
							the small version, because the larger the model, the slower
							inference is, and while performance doesn’t really matter when
							training, one can simply leave their computer running while they
							are not home, it is still vital during inference, because our
							laptops are not on par with our workstations at home.
						</p>

						<p>
							To use our model, we wrote a script named kokanyrecognize at the
							last minute. It wasn’t very performant nor really clean, so we
							spent a significant amount of time working on it. So far it was
							rewritten to use the new model, however it still has a long way to
							go.
						</p>
						<p>Here is the result so far:</p>
						<img
							src="./img/inference.jpg"
							alt="Image showcasing the accuracy of our model"
						/>
					</div>
					<div>
						<h2>Communication</h2>
						<p>
							We interact with our robot using a few custom programs, named
							KókányControl (kokanyctl) and KókányRecognize (kokanyrecognize).
						</p>
						<p>
							KókányControl has a graphical interface for displaying the video
							and the sensor data it receives from Kókánybot. It takes keyboard
							input, and sends commands to the Raspberry Pi. It also recognizes
							QR codes that appear on Kókánybot’s cameras.
						</p>
						<p>
							KókányRecognize was written to reduce the complexity of
							KókányControl, since image recognition functionality is only
							needed in a few runs, and we can just launch KókányRecognize
							whenever we need it. This also enabled us to build KókányControl
							in pure C, since we would have needed to use C++ to build the
							image recognition bits (which uses OpenCV).
						</p>
						<h2>Video and audio streaming</h2>
						<p>
							During tests, operators are only allowed to see the arenas from
							their robot’s point of view. This meant we needed a way to find a
							way to display the video data from the robot’s cameras.
						</p>
						<p>
							We've learnt from our mistakes last year, and have opted for using
							multiple cameras so that we have better peripheral vision while
							controlling Kókánybot.
						</p>
						<p>
							Multimedia related tasks are surprisingly computation heavy when
							one is working with embedded systems. The CPU in the Raspberry Pi
							4B+ is fairly capable, however we also had to consider power draw
							and thermal related problems. We considered several video formats:
							H.265, AV1 and H.264, but in the end we settled on using raw
							frames from our cameras to minimize latency as much as possible,
							since at the 2023 RoboCup, our camera's high latency caused a lot
							of trouble.
						</p>
						<p>
							Linux assigns <code>/dev/videoN</code> to every camera. The kernel
							does not guarantee the order of the camera devices, however we
							must be able to tell them apart in a consistent way. Linux
							provides a way to do this using
							<a href="https://en.wikipedia.org/wiki/Udev">udev</a> rules. We
							gathered the attributes of the cameras using
							<code>udevadm</code>—a standard udev utility—and
							<a
								href="https://github.com/zsoltiv/kokanybot/blob/master/rules/60-camera.rules"
							>
								wrote rules
							</a>
							to assign the <code>/dev/front-camera</code> and
							<code>/dev/rear-camera</code>
							names to our cameras.
						</p>
						<p>
							On the client side, we implemented the decoding of the video data
							using FFmpeg’s libavformat and libavcodec libraries. Rendering the
							video frames was tricky to figure out because pretty much all
							video encoders store pixels in YCbCr colour space, which SDL isn’t
							the best for.
						</p>
						<p>
							The APIs of the libav* libraries are <i>huge</i>. Thankfully we
							only really needed the
							<a
								href="https://ffmpeg.org/doxygen/trunk/group__lavc__encdec.html"
								target="_blank"
								>high level decoding API</a
							>. The
							<a
								href="https://github.com/namndev/FFmpegTutorial/blob/master/learn-ffmpeg-libav-the-hard-way.md"
								target="_blank"
								>Learn FFmpeg libav the Hard Way</a
							>
							tutorial combined with the
							<a
								href="https://git.ffmpeg.org/gitweb/ffmpeg.git/tree/HEAD:/doc/examples"
								target="_blank"
								>examples</a
							>
							in the project's documentation also made things much easier.
						</p>
					</div>
					<div>
						<h2>Human-robot interface</h2>
						<p>Still thinkking</p>
					</div>
				</div>
				<div>
					<h1>Hardware</h1>
					<div>
						<h2>Setup and packing, operation station</h2>
					</div>
					<div>
						<h2>Mission strategy</h2>
					</div>
					<div>
						<h2>Tests and experiments</h2>
					</div>
					<div>
						<h2>Strenght of the robot on the field</h2>
					</div>
					<div>
						<h2>The build and the development process of the robot.</h2>
						<div class="robot-build">
							<h3>The building logics of our robots</h3>
							<p>
								We will demonstrate our build process in this sections. The
								robot itself is divide to four parts.
							</p>
							<ul>
								<li>The body of the robots</li>
								<li>The movement part</li>
								<li>The support arm (we call it Flipper)</li>
								<li>The robot arm</li>
							</ul>
							<div class="wheels">
								<h3>Movement structure</h3>
								<p>
									We were putting a lot of thought in our wheels, because in the
									previous competitions the radius of the wheels weren't enough.
									It always stuck on something high so this time we selected a
									wheels which bigger in radius with that we started the
									development for the rim the process was hard and we had many
									fails in the implementation.
								</p>
								<p>
									The very first version of the wheels had a very big issue. The
									shaft was originaly separated from the rim so the wheels were
									easily switchable but we redesigned it because we saw it
									pointless as we were using the biggest tire possible and
									didn't want to switch to a smaller one so we merged the shaft
									into the rim, however we quickly realised it was a big
									mistake. On the testing phrase the shaft broke on the
									conecting part of the rim on strong impact. So we raised the
									surface on the conntecting part of the rim and the shaft but
									on the competition it broke again.
								</p>
								<div class="fails">
									<div class="img-box">
										<img src="./img/wheels_v2.png" alt="" class="img-compare" />
										<figcaption>The first version</figcaption>
									</div>
									<div class="img-box">
										<img src="./img/wheels_v1.png" alt="" class="img-compare" />
										<figcaption>The second version</figcaption>
									</div>
								</div>
								<p>
									So as we learned our mistakes we separated the shaft and rim
									again. The shaft itself is made out of steel so it's durable
									incase of an impact however the rim is 3D printed for our
									tire. Here u can see the 3D designes of our wheels. The motor
									is JGA25-370 12V and as we searched for motors it is the
									strongest and most suitable for our robot because it gives us
									strenght for a low price. Of course we had to make a holder
									for the motor. In the previous RoboCup we had an issue which
									prevented us for getting more points ia the motor holder. We
									had an L shaped holder which broke during the competition so
									this time we designed a more durable one.
								</p>
								<div class="fails">
									<div class="ing-box">
										<img src="./img/shaft.png" alt="" class="img-compare" />
										<figcaption>The current shaft</figcaption>
									</div>
									<div class="ing-box">
										<img src="./img/rim.png" alt="" class="img-compare" />
										<figcaption>The rim for the shaft</figcaption>
									</div>
								</div>
								<div class="fails">
									<div class="img-box">
										<img
											src="./img/old_motor_holder.png"
											alt=""
											class="img-compare"
										/>
										<figcaption>
											Our previous motor holder in RoboCup23
										</figcaption>
									</div>
									<div class="img-box">
										<img
											src="./img/motor holder.png"
											alt=""
											class="img-compare"
										/>
										<figcaption>Our current motor holders</figcaption>
									</div>
								</div>
								<div class="vid">
									<video
										src="vids/wheel.mp4"
										controls
										width="960"
										height="540s"
									></video>
								</div>
							</div>
							<div class="flipper">
								<h3>Flipper (Support arm)</h3>
								<video
									src="./vids/flipper.mp4"
									width="960"
									height="540s"
									controls
								></video>
							</div>
							<div class="robot-arm">
								<h3>Robot arm</h3>
								<video
									src="./vids/robot arm.mp4"
									width="960"
									height="540s"
									controls
								></video>
							</div>
							<div class="main-parts">
								<h3>The main part of our robot</h3>
								<video
									src="./vids/main base_1.mp4"
									controls
									width="960"
									height="540s"
								></video>
							</div>
						</div>
					</div>
				</div>
				<div>
					<h1>What we learned so far</h1>
					<p>
						In the beginning we used
						<a href="https://www.tinkercad.com/">TinkerCAD</a>
						for 3D modelling, later on we switched to
						<a href="https://www.autodesk.com/products/fusion-360/overview"
							>AutoDesk Fusion360</a
						>
						and gained experience with it.
					</p>
				</div>
				<div>
					<h1>What are we gonna do until the competition</h1>
					<p>
						We are facing an upcoming "event" in our life. In our country this
						time of the year amd age all highschool student will be attending a
						examination which will decide our future. Preparing to the
						competition and at the same time studying to our upcoming exams is
						very difficult so we are trying to divide our time accordingly. Of
						course we want to get better grades and points from the exam as well
						as work more on the robot and test as many of the scenarios that
						could accour during the competition.
					</p>
				</div>
				<div>
					<h1>Software packages and hardware components</h1>
					<div>
						<h2>Software packages</h2>
						<table>
							<tbody>
								<tr class="odd">
									<td>
										<a
											href="https://git.kernel.org/pub/scm/libs/libgpiod/libgpiod.git/about/"
											target="_blank"
										>
											libgpiod
										</a>
									</td>
									<td>DC motor control, sensor control, stepper control</td>
								</tr>
								<tr class="even">
									<td>
										<a href="https://opencv.org/" target="_blank"> OpenCV </a>
									</td>
									<td>Image recognition</td>
								</tr>
								<tr class="odd">
									<td>
										<a href="https://ffmpeg.org/" target="_blank"> FFmpeg </a>
									</td>
									<td>
										Video streaming, Image recognition backend (used by OpenCV)
									</td>
								</tr>
								<tr class="even">
									<td>
										<a href="https://www.libsdl.org/" target="_blank"> SDL2 </a>
									</td>
									<td>
										Used by KókányControl to process keyboard input and display
										video
									</td>
								</tr>
								<tr class="odd">
									<td>
										<a
											href="https://github.com/libsdl-org/SDL_ttf"
											target="_blank"
										>
											SDL2_ttf
										</a>
									</td>
									<td>
										Used by KókányControl to draw text for displaying sensor
										data
									</td>
								</tr>
								<tr class="even">
									<td>
										<a
											href="https://github.com/libsdl-org/SDL_net"
											target="_blank"
										>
											SDL2_net
										</a>
									</td>
									<td>Used by KókányControl to handle networking</td>
								</tr>
								<tr class="odd">
									<td>
										<a href="https://zbar.sourceforge.net/" target="_blank">
											libzbar
										</a>
									</td>
									<td>Used for QR code detection</td>
								</tr>
								<tr class="even">
									<td>
										<a
											href="https://github.com/ultralytics/ultralytics"
											target="_blank"
										>
											YoloV8 (small)
										</a>
									</td>
									<td>The model we use for object detection</td>
								</tr>
								<tr class="even">
									<td>
										<a
											href="https://universe.roboflow.com/new-workspace-xqnz7/rmrc-dqa9p"
											target="_blank"
										>
											RMRC Dataset
										</a>
									</td>
									<td>Dataset used for training our image recognition model</td>
								</tr>
								<tr class="odd">
									<td>
										<a href="https://undefined-medium.com/" target="_blank">
											undefined medium
										</a>
									</td>
									<td>The font used in kokanyctl</td>
								</tr>
								<tr class="even">
									<td>
										<a
											href="https://github.com/zsoltiv/libhwpwm/tree/master"
											target="_blank"
										>
											libhwpwm
										</a>
									</td>
									<td>
										A C library to interface with the
										<a
											href="https://www.kernel.org/doc/html/latest/driver-api/pwm.html#using-pwms-with-the-sysfs-interface"
										>
											Linux PWM userspace API
										</a>
									</td>
								</tr>
							</tbody>
						</table>
					</div>
					<div>
						<h2>hardware Components and estimated prices</h2>
						<table id="partlist">
							<tbody>
								<tr>
									<th>Component</th>
									<th>Cost (in Euros)</th>
								</tr>
								<tr>
									<td>Raspberry Pi 4B+</td>
									<td>71</td>
								</tr>
								<tr>
									<td>4x JGA25-370 12V 60rpm</td>
									<td>34.58</td>
								</tr>
								<tr>
									<td>Parkside X20V Drill Battery</td>
									<td>42.16</td>
								</tr>
								<tr>
									<td>DCDC-6010-M, DC/DC step-down, max. 60V, max. 10A</td>
									<td>17</td>
								</tr>
								<tr>
									<td>Webcamera</td>
									<td>26.78</td>
								</tr>
								<tr>
									<td>2x LT-623</td>
									<td>4.23</td>
								</tr>
								<tr>
									<td>2kg PLA filament for 3D prints</td>
									<td>60.75</td>
								</tr>
								<tr>
									<td>5x Tower Pro MG996R</td>
									<td>44</td>
								</tr>
								<tr>
									<td>Adafruit PCA9685 servo controller</td>
									<td>13,77</td>
								</tr>
								<tr>
									<td>6-24V 12V/24V to 5V 3A CAR USB Charger Modul</td>
									<td>3</td>
								</tr>
								<tr>
									<td>AX SS HYRAX crawler tire 120mm diameter 4pc pack</td>
									<td>30,52</td>
								</tr>
								<tr>
									<td>SZBK07 300W buck converter</td>
									<td>13</td>
								</tr>
								<tr>
									<td>IRISTech W-25</td>
									<td>26</td>
								</tr>
								<tr>
									<td>Overall cost of components</td>
									<td id="totalcost"></td>
								</tr>
							</tbody>
						</table>
					</div>
				</div>
			</div>
		</div>
		<script src="script.js"></script>
	</body>
</html>
